---
title: "Bayesian Causal Inference"
author: Ben Goodrich
date: "`r format(Sys.time(), '%B %d, %Y')`"
autosize: true
urlcolor: blue
header-includes:
   - \usepackage{amsmath}
   - \usepackage{amssymb}
   - \usepackage{color}
   - \usepackage{cancel}
output:
  ioslides_presentation:
    widescreen: yes
    highlight: pygment
editor_options: 
  chunk_output_type: console
---
<style type="text/css">
slides > slide:not(.nobackground):after {
  content: '';
}
</style>

<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>

```{r setup, include=FALSE}
options(width = 90, scipen = 1)
library(knitr)
library(dplyr)
library(ggplot2)
opts_chunk$set(echo = TRUE)
knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar = c(4, 4, .1, .1), las = 1)  # smaller margin on top and right
})
set.seed(20220228)
options(mc.cores = parallel::detectCores())
```

\newcommand{\ind}{\perp\!\!\!\!\perp} 

## HW1 Again

- What was estimated GDP growth in the first quarter?

> - [T]he conduct of monetary policy in the United States has come to involve, at its core, crucial elements of risk management. This conceptual framework emphasizes understanding as much as possible the many sources of risk and uncertainty that policymakers face, quantifying those risks when possible, and assessing the costs associated with each of the risks. In essence, the risk-management approach to monetary policymaking is an application of Bayesian decision-making. 

> - Alan Greenspan, 2004, while chairing the Federal Reserve Board

> - Why is the Bayesian approach not taught in economics, public policy, etc.?

> - Why, if you search for "Bayesian" in the course descriptions on Albert, does little turn up?

## HW2, Question 1 {.build}

<div class="columns-2">
![](https://images.squarespace-cdn.com/content/v1/5150aec6e4b0e340ec52710a/1364352051365-HZAS3CLBF7ABLE3F5OBY/Data_Science_VD.png)

> - What do you get if you equate $\dots$
> - <div class="green2"> Probability </div>
> - <div class="blue2"> Priors, DAGs, and Model Building</div>
> - <div class="red2"> Markov Chain Monte Carlo </div>
> - Since at least $1990$, Bayesian estimation has
  been what data science purported to be but isn't
> - Data science came along later but largely ignored Bayesian approaches
> - In reality, data science programs are asymmetric with more
  emphasis on hacking skills and less on the others
</div>

## HW2, Question 2

- Could largely use negative binomial example from Week4
- With GLMs, it is very easy to put positive prior probability on extreme events
- Drawing from the prior predictive distribution has a tendency to overflow, in
  which case you can do something like this in `generated quantities`
```{stan output.var="NB", eval = FALSE}
vector[N] eta = alpha + offset + X * beta;
for (n in 1:N) {
  real inv_mu = exp(-eta[n]);
  real lambda = gamma_rng(phi, phi * inv_mu);
  if (lambda > poisson_max) { // 2^30
    y_rep[n] = 2 * (poisson_max - 1);
  } else {
    y_rep[n] = poisson_rng(lambda);
  }
}
```

> - More scientifically accurate videos have fewer views

## HW2, Question 3

- Could model vaccination with a normal (percentage), beta (proportion),
  or binomial (count) distribution
- Should allow heterogeneity in the data-generating process by state
- $\alpha_j \thicksim \mathcal{N}\left(\mu_\alpha, \sigma\right)$ works
  better --- in this case --- than $\alpha_j \equiv \mu_\alpha + \sigma a_j$
  with $a_j \thicksim \mathcal{N}\left(0,1\right)$ because the state-wise
  heterogeneity is large relative to the county-level error

## Graphical Causal Models Framework

- Potential Outcomes framework is widespread in economics & political science
  and is fine for obtaining point estimates of Average Causal Effects (ACEs) in
  simple experiments. It only involves probability theory and algebra / calculus.
- Graphical Causal Models framework is widespread in epidemiology and to a lesser extent,
  in sociology and psychology but is overkill for estimating ACEs in simple experiments
- A theorem in one framework implies a theorem in the other framework
- Directed Acyclic Graphs can serve multiple purposes:

  1. A language to communicate theories
  2. Identification Analysis: Whether a theory implies the ACE could be calculated in a 
    population
  3. Testable Implications: What observable variables are conditionally independent,
    which can be investigated with data

## Directed Acyclic Graphs (DAGs)

- Three elements to a DAG:

  1. Variables / Nodes
  2. Arrows from an earlier (in time) node to a later node
  3. Absence of arrows between nodes, which implies an ACE is zero
  
- $A \rightarrow B$ means that if $A$ were experimentally manipulated there MAY be a
  non-zero ACE that is assumed to be unmediated by any other variable
- Cannot start at any node, follow arrows, and get back to where you started
- DAGs are usually written w/o distributional or functional form assumptions

## Fork {.columns-2}

- $X\leftarrow\boxed{{Z}}\rightarrow Y$
- Like GDP* driving both GDI and GDP
- Stratifying on $Z$ implies $X \perp Y \mid Z$
- Not stratifying on $Z$ implies $X$ and $Y$ are marginally dependent, i.e
  $f\left(\bcancel{z} \bigcap x \bigcap y\right) \neq f\left(x\right) 
  \times f\left(y\right)$
```{r, fig.width=5, fig.height=5, message=FALSE}
tibble(z = rbinom(10^4, size = 1, 
                  prob = 0.5),
       x = rnorm( 10^4, mean =  z),
       y = rnorm( 10^4, mean = -z)) %>%
  ggplot(aes(x, y)) + geom_point() + 
  facet_wrap(~as.factor(z), ncol = 1) +
  geom_smooth(method = "lm", color = "red")
```

## Pipe (or chain) {.columns-2}

- $Z\overset{\beta_{1}}{\rightarrow} X \overset{\beta_{2}}{\rightarrow}Y$
- Stratifying on $X$ implies $Z \perp Y \mid X$
- Not stratifying on $X$ implies $Z$ and $Y$ are marginally dependent, i.e
  $f\left(z \bigcap \bcancel{x} \bigcap y\right) \neq f\left(z\right) \times 
  f\left(y\right)$
- Iff the relationships are linear the marginal effect of $Z$
  on $Y$ is $\beta_1 \beta_2$
```{r, fig.width=5, fig.height=5, message = FALSE}
tibble(z = rnorm( 10^4),
       x = rbinom(10^4, size = 1, 
                  prob = pnorm(z)),
       y = rnorm( 10^4, mean = x)) %>%
  ggplot(aes(z, y)) + geom_point() + 
  facet_wrap(~as.factor(x), ncol = 1) +
  geom_smooth(method = "lm", color = "red")
```

## Collider {.columns-2}

- $X\rightarrow Z \leftarrow Y$
- Not stratifying on $Z$ implies $X$ and $Y$ are marginally independent, i.e
  $f\left(x \bigcap \bcancel{z} \bigcap y\right) = f\left(x\right) \times 
  f\left(y\right)$
- Stratifying on $Z$ implies $X$ and $Y$ are conditionally dependent, i.e
  $f\left(x \bigcap y \mid z\right) \neq f\left(x \mid z\right) \times 
  f\left(y \mid z \right)$
```{r, fig.width=5, fig.height=5, message = FALSE}
tibble(x = rnorm( 10^4),
       y = rnorm( 10^4),
       z = rbinom(10^4, size = 1,
                  prob = pnorm(x - y))) %>%
  ggplot(aes(x, y)) + geom_point() + 
  facet_wrap(~as.factor(z), ncol = 1) +
  geom_smooth(method = "lm", color = "red")
```

## Descendant {.columns-2}

- $Z\overset{\beta_{1}}{\rightarrow} X \overset{\beta_{2}}{\rightarrow}Y \rightarrow R$
- Not stratifying on $R$ implies effect of $Z$ on $Y$ is $\beta_1 \beta_2$ (assuming
  linearity)
- Stratifying on $R$ prevents you from recovering $\beta_1 \beta_2$ via regression
```{r, fig.width=5, fig.height=5, message = FALSE}
tibble(z = rnorm( 10^4),
       x = rnorm( 10^4, mean = z),
       y = rnorm( 10^4, mean = x),
       r = rbinom(10^4, size = 1,
                  prob = pnorm(y / 2))) %>%
  ggplot(aes(z, y)) + geom_point() + 
  facet_wrap(~as.factor(r), ncol = 1) + 
  geom_smooth(method = "lm", color = "red")
```


## Blocking / Closing a Path and d-separation

- A path is a sequence of connected nodes, regardless of the arrows' direction

  1. Causal Path: A path that exclusively follows the arrows
  2. Non-causal Path: Any other path

- To block a path, either

  1. Condition on a noncollider along a path: $A \leftarrow \fbox{C} \rightarrow B$
  2. Refrain from conditioning on a (descendant of a) collider along a path: $A \rightarrow C \leftarrow B$

> - Two variables are "d-separated" iff all paths between them are blocked by conditioning on a
  possibly empty set of variables $\{Z\}$, in which case they are conditionally independent
> - Two variables that are not "d-separated" are not conditionally independent

## Adjustment and Backdoor Criteria

- Adjustment Criterion is satisfied iff

  1. All non-causal paths from $X$ to $Y$ blocked by a (possibly empty) set of variables $\{Z\}$
  2. No variable in $\{Z\}$ lies on or descends from any causal path from $X$ to $Y$

- If the adjustment criterion is satisfied, ACE of $X$ on $Y$ is identified and can be
  consistently estimated with a (possibly weighted) difference in means
  
- Backdoor Criterion is satisfied iff

  1. No element of $\{Z\}$ is a descendant of $Y$
  2. Some element of $\{Z\}$ blocks all "backdoor" paths, i.e. those starting with $\rightarrow X$
  
- Backdoor Criterion implies Adjustment Criterion but not vice versa

## Dagitty

- There is a [website](www.dagitty.net), that implements the most common and useful identification
  strategies for DAGs and a similar R package, dagitty:
```{r, small.mar = FALSE, fig.height=3, fig.width=10, message = FALSE}
library(dagitty); g <- dagitty( "dag{ x -> y ; x <-> m <-> y }" )
library(ggdag); ggdag(g) + theme_void()
```

## Double-headed Arrows

- The dagitty R package and a lot of authors use $A \leftrightarrow B$ as a shorthand
  for a $A \leftarrow U \rightarrow B$ with $U$ unobserved or for
  $\begin{matrix}X & \leftrightarrow & Y \\
                 \downarrow & \swarrow &\\
                 \fbox{C}   &             & \end{matrix}$ due to conditioning
- Any path involving a $\leftrightarrow$ is a non-causal path

## Contrasts with Common Practice

- Supervised learning generally does not utilize DAGs and is not capable of estimating causal
  effects. Models are scored on how well they predict the outcome (in the testing set), which
  is better if you condition on descendants of the outcome, mediators, some colliders, etc.
- Regressions generally do not estimate the causal effect of all covariates and rarely
  estimate the causal effect of any covariate. They implicitly correspond to a DAG like
  (although the error term is often omitted):
$$\begin{matrix} & \nearrow \fbox{X}_1 & \searrow &  \\
              C  & \rightarrow \fbox{X}_2 & \rightarrow & Y\leftarrow \epsilon\\
                 & \searrow \fbox{X}_3 & \nearrow & \end{matrix}$$
- Non-Bayesians tend to write DAGs without distributions, parameters, & priors

## CausalQueries R Package

- All DAGs where all observed nodes are binary have a (multinomial) likelihood function
  that can be combined with priors on the "type" parameters to perform Bayesian inference
- The CausalQueries R Package takes a DAG, priors, and data and uses Stan to draw from the
  posterior distribution of the parameters given the data & priors
- Useful for describing your beliefs about (not necessarily average) causal effects in
  many situations that are not simple experiments
  
## Dataset from Bertrand and Mullainathan (2004)

- Resumes were created for a fictitious person applying for an entry-level job but
  the name at the top of the resume was randomized to make the company think the applicant 
  was probably black / white / male / female. The outcome is whether the company called the
  applicant to schedule an interview, etc.
```{r, message = FALSE}
data(resume, package = "qss") # remotes::install_github("kosukeimai/qss-package")
resume <- mutate(resume, white = race == "white", female = sex == "female")
resume_grouped <- group_by(resume, white, female, call) %>% 
  summarize(n = n())
glimpse(resume_grouped)
```

## Basic Frequentist Inference

```{r}
prop.test(matrix(rev(resume_grouped$n), ncol = 2, byrow = TRUE,
                 dimnames = list(c("white_female", "white_male",
                                   "black_female", "black_male"),
                                 c("call", "no call"))))
```

> - Could get point estimates, but many questions prohibited by Frequentism, 
  e.g. "How sure are you that companies favor white / male over black / female?"

## Principal (not principled) Stratification {.smaller}

- If $X$ and $Y\left(X\right)$ are both binary, then there are $2^2 = 4$ types of observations:

Function                        | Rubin         | Synonym         | This Example
------------------------------- | ------------- | --------------- | -------------------
$Y\left(x\right) \neq x$        | Defiers       | **A**dverse     | Interview iff white
$Y\left(x\right) = x$           | Compliers     | **B**eneficial  | Interview iff black
$Y\left(x\right) = 0 \forall x$ | Never takers  | **C**hronic     | No interview regardless
$Y\left(x\right) = 1 \forall x$ | Always takers | **D**estined    | Interview regardless

$\downarrow$ What We Observe $\rightarrow$ | No interview   | Interview
------------------------------------------ | -------------- | ---------
White          | $\implies$ **B** or **C** | $\implies$ **A** or **D**
Black          | $\implies$ **A** or **C** | $\implies$ **B** or **D**

## Dirichlet Distribution

- Dirichlet distribution is over the parameter space of PMFs --- i.e. $\pi_k \geq 0$ and 
  $\sum_{k = 1}^K \pi_k = 1$ --- and the Dirichlet PDF is
$f\left(\boldsymbol{\pi} \mid \boldsymbol{\alpha}\right) = \frac{1}{B\left(\boldsymbol{\alpha}\right)}\prod_{k=1}^{K}\pi_{k}^{\alpha_{k}-1}$
where $\alpha_{k}\geq0\,\forall k$ and the multivariate Beta
function is $B\left(\boldsymbol{\alpha}\right)=\frac{\prod_{k=1}^{K}\Gamma\left(\alpha_{k}\right)}{\Gamma\left(\prod_{k=1}^{K}\alpha_{k}\right)}$
where $\Gamma\left(z\right)= \frac{1}{z} \prod_{n = 1}^\infty \frac{\left(1 + \frac{1}{n}\right)^n}
{1 + \frac{z}{n}} = \int_{0}^{\infty}u^{z-1}e^{-u}du$ is the Gamma function
- $\mathbb{E}\pi_{i}=\frac{\alpha_{i}}{\sum_{k=1}^{K}\alpha_{k}}\,\forall i$
and the mode of $\pi_{i}$ is $\frac{\alpha_{i}-1}{-1+\sum_{k=1}^{K}\alpha_{k}}$
if $\alpha_{i}>1$
- Iff $\alpha_{k}=1\,\forall k$, $f\left(\left.\boldsymbol{\pi}\right|\boldsymbol{\alpha}=\mathbf{1}\right)$
is constant over $\Theta$ (simplexes)
- Beta distribution is a special case of the Dirichlet where $K = 2$
- Marginal and conditional distributions for subsets of $\boldsymbol{\pi}$ are also Dirichlet

## Conditioning on the Resume Data

```{r, message = FALSE}
library(CausalQueries)
model <- make_model("white -> call <- female")
model %>% get_parameter_matrix %>% nrow # default priors are flat Dirichlet
```

```{r, resume, cache = TRUE, results="hide", message = FALSE}
model <- update_model(model, seed = 12345, data = resume)
```
```{r, query1, cache = TRUE}
query_model(model, using = "posteriors", queries = # can also ask about priors
              c(race_ACE = "call[white  = 0] - call[white  = 1]", 
                sex_ACE  = "call[female = 0] - call[female = 1]",
                Pr = "call[white = 0, female = 1] > call[white = 1, female = 1]"))
```

## Counterfactual Distributional Queries

```{r, query2, cache = TRUE, fig.height=4, fig.width=10, message = FALSE}
query_distribution(model, using = "posteriors", given = "white == 0", # condition
                   query = "call[white = 1]") %>% as.data.frame %>%   # counterfactual
  ggplot() + geom_density(aes(`.`)) + xlab("Probability of interview")
```

## A Model with Confounding {.build}

```{r, fig.width=8, fig.height=2.5, small.mar = FALSE}
model <- make_model("W <- X -> Y") %>% set_confound(list("W <-> X", "W <-> Y"))
CausalQueries:::translate_dagitty(model) %>% ggdag + theme_void()
```
```{r}
adjustmentSets(CausalQueries:::translate_dagitty(model), exposure = "X", outcome = "Y")
```

## Conditional vs. Joint Modeling

- If you do not condition on $W$ in the previous model, the ACE of $X$ on $Y$
  is identified and easy to estimate via regression, differences in means, etc.
- If you do condition on $W$, the ACE of $X$ on $Y$ is not identified and
  cannot be estimated correctly via regression, differences in means, etc.
- If you use CausalQueries, the model is of $W$, $X$, and $Y$ jointly, which
  does not condition on the collider, $W$
- You can subsequently do things like
```{r, eval = FALSE}
query_distribution(model, using = "posteriors", given = "W == 0", # condition
                   query = "Y[X = 1] - Y[X = 0]") # conditional ACE
```

## Instrumental Variables with MTO Study

```{r, fig.height=4.5, fig.width=10}
dagify(employed ~ moved + u, moved ~ voucher + u, exposure = "moved", outcome = "employed",
       latent = "u") %>%  ggdag_instrumental(text_size = 2.75) + theme_void()
```

## Frequentist Instrumental Variable Estimation

- When everything is binary, can point estimate the (local) average treatment effect
  of moving (because of the MTO voucher) as
$$\widehat{\beta} = \frac{\mathrm{cov}\left(\text{voucher, employed}\right)}
{\mathrm{cov}\left(\text{voucher, moved}\right)}$$

> - The sampling distribution of this Wald estimator across datasets of size $N$
  can be terrible because both the numerator and the denominator are both real 
  random variables
> - Cauchy distribution is the ratio of standard normal variates and has no
  expectation; Frequentist instrumental variable estimators are similar
> - Major problem if you want to pretend MLEs characterize a normal posterior
  distribution
> - Actual posterior distribution does not involve any ratios, although it
  may be far from normal

## Angrist and Kreuger (1991) Data

```{r}
ROOT <- "http://higheredbcs.wiley.com/legacy/college/"
PATH <- "lancaster/1405117206/datasets/AKdata.zip"
if (!file.exists("AKdata.zip")) {
  download.file(paste0(ROOT, PATH), destfile = "AKdata.zip")
  unzip("AKdata.zip")  
}
AKdata <- read.table("AKdata.txt", header = FALSE, skip = 4,
                     col.names = c("ID", "log_wage", "schooling", "birth_quarter", "age"))
glimpse(AKdata)
```

## Generative Model from McElreath section 14.3

- "The answer is actually pretty simple. We just use the generative model."
<div class="columns-2">
$$q_n \thicksim \mathcal{C}\left(\left[\frac{1}{4},\frac{1}{4},\frac{1}{4},\frac{1}{4}\right]^\top\right)\\
\left[\lambda_1,\lambda_2, \lambda_3,\lambda_4\right]^\top \thicksim ???\\
\sigma_s \thicksim ???\\
s_n \thicksim \mathcal{N}\left(\lambda_{q_n}, \sigma_s\right)$$
$$\sigma_u \thicksim ???\\
u_n \thicksim \mathcal{N}\left(0, \sigma_u\right)\\
\alpha \thicksim ???\\
\beta \thicksim ???\\
\mu_n \equiv \alpha + \beta s_n + u_n\\
\sigma_y \thicksim ???\\
y_n \thicksim \mathcal{N}\left(\mu_n, \sigma_y\right)$$
</div>

> - $s_n$ and $y_n$ are marginally bivariate normal due to the common latent $u_n$,
  which we can integrate out analytically in this case (after introducing $\rho$)
  
## Stan Program {.smaller}

<div class="columns-2">
```{r, echo = FALSE, comment = ""}
writeLines(readLines("iv.stan"))
```
</div>

## Calling the Stan Program

```{r}
stan_data <- with(AKdata, list(N = nrow(AKdata), q = birth_quarter, 
                               s = schooling - mean(schooling), y = log_wage, 
                               prior_only = FALSE, m = c(0, log(400), 0), 
                               scale = c(1, 3, 1), r = 1))
```
```{r, iv, cache = TRUE, results = "hide"}
post_iv <- stan("iv.stan", data = stan_data, seed = 20220504)
```

